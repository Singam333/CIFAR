{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XzjjkWAY8M0b",
    "outputId": "b89b697f-58e1-4ad1-fee0-56c8af81d375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FDR8oBR8bb7"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 16\n",
    "num_filter = 24\n",
    "compression = 0.5\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "XBXayo998i2t",
    "outputId": "c8580da7-b537-4fc9-cec5-3ccea69bc30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLHQZLECifKA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
    "X_train_std = np.std(X_train, axis=(0,1,2))\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjqGiqu98mvA"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter =32):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter =32):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    conv=layers.Conv2D(int(10),(2,2), use_bias=False)(AvgPooling)\n",
    "    conv1=layers.Conv2D(int(10),(1,1), use_bias=False)(conv)\n",
    "    flat = layers.Flatten()(conv1)\n",
    "    output = layers.Activation(\"softmax\")(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMDuzqQX8o_y"
   },
   "outputs": [],
   "source": [
    "num_filter = 24\n",
    "l = 16\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (4,4), use_bias=False ,padding='same',activation=tf.nn.relu,kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter)\n",
    "First_Transition = transition(First_Block, num_filter )\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter )\n",
    "Second_Transition = transition(Second_Block, num_filter )\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter )\n",
    "Third_Transition = transition(Third_Block, num_filter )\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter )\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2JPH7_5x8skc",
    "outputId": "2664f7c8-6331-493b-b21e-be68ac56566f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 24)   1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 12)   2592        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 36)   0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 12)   3888        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 48)   0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 12)   5184        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 60)   0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 60)   240         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 60)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 12)   6480        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 72)   0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 72)   288         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 72)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 12)   7776        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 84)   0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 84)   336         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 84)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 12)   9072        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 96)   0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 96)   384         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 96)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 12)   10368       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 108)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 108)  432         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 108)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 12)   11664       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 120)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 120)  480         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 120)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 12)   12960       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 132)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 132)  528         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 132)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 12)   14256       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 144)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 144)  576         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 144)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 12)   15552       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 156)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 156)  624         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 156)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 12)   16848       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 168)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 168)  672         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 168)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 12)   18144       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 180)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 180)  720         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 180)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 12)   19440       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 192)  0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 192)  768         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 192)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 12)   20736       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 204)  0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 204)  816         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 204)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 12)   22032       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 216)  0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 216)  864         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 216)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 12)   2592        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 12)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 12)   48          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 12)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 12)   1296        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 24)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 24)   96          concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 24)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 12)   2592        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 36)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 36)   144         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 36)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 12)   3888        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 48)   0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 48)   192         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 12)   5184        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 60)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 60)   240         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 60)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 12)   6480        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 72)   0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 72)   288         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 72)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 12)   7776        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 84)   0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 84)   336         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 84)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 12)   9072        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 96)   0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 96)   384         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 12)   10368       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 108)  0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 108)  432         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 108)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 12)   11664       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 120)  0           concatenate_23[0][0]             \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 120)  480         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 120)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 12)   12960       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 16, 16, 132)  0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 132)  528         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 132)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 12)   14256       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 144)  0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 144)  576         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 144)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 12)   15552       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 156)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 156)  624         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 156)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 12)   16848       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 168)  0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 168)  672         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 168)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 12)   18144       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 180)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 180)  720         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 180)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 12)   19440       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 192)  0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 192)  768         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 192)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 12)   20736       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 204)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 204)  816         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 204)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 12)   2448        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 12)     0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 12)     48          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 12)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 12)     1296        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 24)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 24)     96          concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 24)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 12)     2592        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 36)     0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 36)     144         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 36)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 12)     3888        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 48)     0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 48)     192         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 48)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 12)     5184        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 60)     0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 60)     240         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 60)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 12)     6480        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 72)     0           concatenate_35[0][0]             \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 72)     288         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 72)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 12)     7776        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 8, 8, 84)     0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 84)     336         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 84)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 12)     9072        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 8, 8, 96)     0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 96)     384         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 96)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 12)     10368       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 8, 8, 108)    0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 108)    432         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 108)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 12)     11664       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 120)    0           concatenate_39[0][0]             \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 120)    480         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 120)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 12)     12960       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 8, 8, 132)    0           concatenate_40[0][0]             \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 132)    528         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 132)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 12)     14256       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 8, 8, 144)    0           concatenate_41[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 144)    576         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 144)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 12)     15552       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 156)    0           concatenate_42[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 156)    624         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 156)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 12)     16848       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 168)    0           concatenate_43[0][0]             \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 168)    672         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 168)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 12)     18144       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 180)    0           concatenate_44[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 180)    720         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 180)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 12)     19440       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 192)    0           concatenate_45[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 192)    768         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 192)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 12)     20736       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 204)    0           concatenate_46[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 204)    816         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 8, 8, 204)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 12)     2448        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 12)     0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 12)     48          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 12)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 12)     1296        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 4, 4, 24)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 24)     96          concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 24)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 12)     2592        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 4, 4, 36)     0           concatenate_48[0][0]             \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 36)     144         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 36)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 12)     3888        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 4, 4, 48)     0           concatenate_49[0][0]             \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 48)     192         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 48)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 4, 4, 12)     5184        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 4, 4, 60)     0           concatenate_50[0][0]             \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 60)     240         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 4, 4, 60)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 4, 4, 12)     6480        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 4, 4, 72)     0           concatenate_51[0][0]             \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 72)     288         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 4, 4, 72)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 4, 4, 12)     7776        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 4, 4, 84)     0           concatenate_52[0][0]             \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 84)     336         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 4, 4, 84)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 4, 4, 12)     9072        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 4, 4, 96)     0           concatenate_53[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 4, 4, 96)     384         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 4, 4, 96)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 4, 4, 12)     10368       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 4, 4, 108)    0           concatenate_54[0][0]             \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4, 4, 108)    432         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 4, 4, 108)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 4, 4, 12)     11664       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 4, 4, 120)    0           concatenate_55[0][0]             \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 4, 4, 120)    480         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 120)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 4, 4, 12)     12960       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 4, 4, 132)    0           concatenate_56[0][0]             \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 4, 4, 132)    528         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 132)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 4, 4, 12)     14256       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 4, 4, 144)    0           concatenate_57[0][0]             \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 4, 4, 144)    576         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 4, 4, 144)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 4, 4, 12)     15552       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 4, 4, 156)    0           concatenate_58[0][0]             \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 156)    624         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 156)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 4, 4, 12)     16848       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 4, 4, 168)    0           concatenate_59[0][0]             \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 168)    672         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 168)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 12)     18144       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 4, 4, 180)    0           concatenate_60[0][0]             \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4, 4, 180)    720         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 180)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 12)     19440       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 4, 4, 192)    0           concatenate_61[0][0]             \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 4, 4, 192)    768         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 192)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 12)     20736       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 4, 4, 204)    0           concatenate_62[0][0]             \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 4, 4, 204)    816         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 4, 4, 204)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 204)    0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 1, 1, 10)     8160        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 1, 1, 10)     100         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10)           0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 10)           0           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 772,852\n",
      "Trainable params: 757,756\n",
      "Non-trainable params: 15,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Iy6GJ4h8wLQ"
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3,rescale=1./255.)\n",
    "train_generator=datagen.flow(X_train,y_train,batch_size=32)\n",
    "test_generator=datagen.flow(X_test,y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1-bVtF7nubCu",
    "outputId": "cbcf43d9-e1f9-418b-bb07-f6d2dfa712f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1562 steps, validate for 312 steps\n",
      "Epoch 1/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.6743 - accuracy: 0.3859\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43590, saving model to training_1/cp-0001.ckpt\n",
      "1562/1562 [==============================] - 112s 72ms/step - loss: 1.6741 - accuracy: 0.3860 - val_loss: 1.5516 - val_accuracy: 0.4359\n",
      "Epoch 2/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2963 - accuracy: 0.5330\n",
      "Epoch 00002: val_accuracy improved from 0.43590 to 0.57993, saving model to training_1/cp-0002.ckpt\n",
      "1562/1562 [==============================] - 97s 62ms/step - loss: 1.2962 - accuracy: 0.5330 - val_loss: 1.1947 - val_accuracy: 0.5799\n",
      "Epoch 3/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.0850 - accuracy: 0.6121\n",
      "Epoch 00003: val_accuracy improved from 0.57993 to 0.64403, saving model to training_1/cp-0003.ckpt\n",
      "1562/1562 [==============================] - 96s 61ms/step - loss: 1.0848 - accuracy: 0.6121 - val_loss: 1.0146 - val_accuracy: 0.6440\n",
      "Epoch 4/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.9488 - accuracy: 0.6662\n",
      "Epoch 00004: val_accuracy did not improve from 0.64403\n",
      "1562/1562 [==============================] - 96s 61ms/step - loss: 0.9487 - accuracy: 0.6662 - val_loss: 1.0888 - val_accuracy: 0.6171\n",
      "Epoch 5/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8640 - accuracy: 0.6965\n",
      "Epoch 00005: val_accuracy improved from 0.64403 to 0.68980, saving model to training_1/cp-0005.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.8640 - accuracy: 0.6965 - val_loss: 0.8905 - val_accuracy: 0.6898\n",
      "Epoch 6/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.7970 - accuracy: 0.7215\n",
      "Epoch 00006: val_accuracy improved from 0.68980 to 0.69952, saving model to training_1/cp-0006.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.7971 - accuracy: 0.7215 - val_loss: 0.8704 - val_accuracy: 0.6995\n",
      "Epoch 7/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.7442 - accuracy: 0.7386\n",
      "Epoch 00007: val_accuracy did not improve from 0.69952\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.7445 - accuracy: 0.7385 - val_loss: 0.8545 - val_accuracy: 0.6951\n",
      "Epoch 8/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.7547\n",
      "Epoch 00008: val_accuracy improved from 0.69952 to 0.74199, saving model to training_1/cp-0008.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.6980 - accuracy: 0.7548 - val_loss: 0.7402 - val_accuracy: 0.7420\n",
      "Epoch 9/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.6604 - accuracy: 0.7709\n",
      "Epoch 00009: val_accuracy improved from 0.74199 to 0.75130, saving model to training_1/cp-0009.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.6604 - accuracy: 0.7710 - val_loss: 0.7072 - val_accuracy: 0.7513\n",
      "Epoch 10/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.7817\n",
      "Epoch 00010: val_accuracy improved from 0.75130 to 0.76773, saving model to training_1/cp-0010.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.6292 - accuracy: 0.7816 - val_loss: 0.6844 - val_accuracy: 0.7677\n",
      "Epoch 11/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.6077 - accuracy: 0.7883\n",
      "Epoch 00011: val_accuracy did not improve from 0.76773\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.6076 - accuracy: 0.7884 - val_loss: 0.6795 - val_accuracy: 0.7672\n",
      "Epoch 12/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.7976\n",
      "Epoch 00012: val_accuracy did not improve from 0.76773\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.5854 - accuracy: 0.7976 - val_loss: 0.6933 - val_accuracy: 0.7638\n",
      "Epoch 13/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.8044\n",
      "Epoch 00013: val_accuracy improved from 0.76773 to 0.77504, saving model to training_1/cp-0013.ckpt\n",
      "1562/1562 [==============================] - 94s 60ms/step - loss: 0.5619 - accuracy: 0.8044 - val_loss: 0.6462 - val_accuracy: 0.7750\n",
      "Epoch 14/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.8117\n",
      "Epoch 00014: val_accuracy improved from 0.77504 to 0.78145, saving model to training_1/cp-0014.ckpt\n",
      "1562/1562 [==============================] - 95s 61ms/step - loss: 0.5397 - accuracy: 0.8118 - val_loss: 0.6340 - val_accuracy: 0.7815\n",
      "Epoch 15/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.5226 - accuracy: 0.8201\n",
      "Epoch 00015: val_accuracy did not improve from 0.78145\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.5226 - accuracy: 0.8201 - val_loss: 0.6318 - val_accuracy: 0.7794\n",
      "Epoch 16/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.5063 - accuracy: 0.8250\n",
      "Epoch 00016: val_accuracy improved from 0.78145 to 0.79858, saving model to training_1/cp-0016.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.5063 - accuracy: 0.8250 - val_loss: 0.5950 - val_accuracy: 0.7986\n",
      "Epoch 17/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4920 - accuracy: 0.8290\n",
      "Epoch 00017: val_accuracy did not improve from 0.79858\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4921 - accuracy: 0.8290 - val_loss: 0.5783 - val_accuracy: 0.7982\n",
      "Epoch 18/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.8351\n",
      "Epoch 00018: val_accuracy did not improve from 0.79858\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4728 - accuracy: 0.8352 - val_loss: 0.6243 - val_accuracy: 0.7883\n",
      "Epoch 19/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4584 - accuracy: 0.8402\n",
      "Epoch 00019: val_accuracy improved from 0.79858 to 0.82352, saving model to training_1/cp-0019.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4584 - accuracy: 0.8402 - val_loss: 0.5267 - val_accuracy: 0.8235\n",
      "Epoch 20/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.8437\n",
      "Epoch 00020: val_accuracy did not improve from 0.82352\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4488 - accuracy: 0.8437 - val_loss: 0.5901 - val_accuracy: 0.8006\n",
      "Epoch 21/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.8470\n",
      "Epoch 00021: val_accuracy did not improve from 0.82352\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.4367 - accuracy: 0.8470 - val_loss: 0.5544 - val_accuracy: 0.8089\n",
      "Epoch 22/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4332 - accuracy: 0.8494\n",
      "Epoch 00022: val_accuracy did not improve from 0.82352\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.4331 - accuracy: 0.8494 - val_loss: 0.5832 - val_accuracy: 0.8018\n",
      "Epoch 23/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8512\n",
      "Epoch 00023: val_accuracy did not improve from 0.82352\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4253 - accuracy: 0.8512 - val_loss: 0.5450 - val_accuracy: 0.8166\n",
      "Epoch 24/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4090 - accuracy: 0.8559\n",
      "Epoch 00024: val_accuracy improved from 0.82352 to 0.83263, saving model to training_1/cp-0024.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4089 - accuracy: 0.8559 - val_loss: 0.4913 - val_accuracy: 0.8326\n",
      "Epoch 25/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.4023 - accuracy: 0.8593\n",
      "Epoch 00025: val_accuracy improved from 0.83263 to 0.83464, saving model to training_1/cp-0025.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.4022 - accuracy: 0.8593 - val_loss: 0.4858 - val_accuracy: 0.8346\n",
      "Epoch 26/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8634\n",
      "Epoch 00026: val_accuracy did not improve from 0.83464\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3924 - accuracy: 0.8634 - val_loss: 0.5147 - val_accuracy: 0.8279\n",
      "Epoch 27/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8663\n",
      "Epoch 00027: val_accuracy did not improve from 0.83464\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3855 - accuracy: 0.8662 - val_loss: 0.5589 - val_accuracy: 0.8138\n",
      "Epoch 28/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8676\n",
      "Epoch 00028: val_accuracy did not improve from 0.83464\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3770 - accuracy: 0.8676 - val_loss: 0.5711 - val_accuracy: 0.8083\n",
      "Epoch 29/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8675\n",
      "Epoch 00029: val_accuracy improved from 0.83464 to 0.84044, saving model to training_1/cp-0029.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.3751 - accuracy: 0.8675 - val_loss: 0.4642 - val_accuracy: 0.8404\n",
      "Epoch 30/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8747\n",
      "Epoch 00030: val_accuracy improved from 0.84044 to 0.84095, saving model to training_1/cp-0030.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.3576 - accuracy: 0.8746 - val_loss: 0.4587 - val_accuracy: 0.8409\n",
      "Epoch 31/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3558 - accuracy: 0.8752\n",
      "Epoch 00031: val_accuracy did not improve from 0.84095\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.3558 - accuracy: 0.8752 - val_loss: 0.4811 - val_accuracy: 0.8343\n",
      "Epoch 32/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8770\n",
      "Epoch 00032: val_accuracy did not improve from 0.84095\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3523 - accuracy: 0.8770 - val_loss: 0.5013 - val_accuracy: 0.8291\n",
      "Epoch 33/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3459 - accuracy: 0.8794\n",
      "Epoch 00033: val_accuracy did not improve from 0.84095\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3459 - accuracy: 0.8795 - val_loss: 0.5098 - val_accuracy: 0.8336\n",
      "Epoch 34/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8804\n",
      "Epoch 00034: val_accuracy did not improve from 0.84095\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3405 - accuracy: 0.8804 - val_loss: 0.4793 - val_accuracy: 0.8363\n",
      "Epoch 35/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8816\n",
      "Epoch 00035: val_accuracy improved from 0.84095 to 0.85296, saving model to training_1/cp-0035.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3368 - accuracy: 0.8817 - val_loss: 0.4338 - val_accuracy: 0.8530\n",
      "Epoch 36/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8850\n",
      "Epoch 00036: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3281 - accuracy: 0.8851 - val_loss: 0.4678 - val_accuracy: 0.8416\n",
      "Epoch 37/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8867\n",
      "Epoch 00037: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3241 - accuracy: 0.8867 - val_loss: 0.5226 - val_accuracy: 0.8213\n",
      "Epoch 38/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8894\n",
      "Epoch 00038: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3175 - accuracy: 0.8894 - val_loss: 0.4745 - val_accuracy: 0.8395\n",
      "Epoch 39/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8889\n",
      "Epoch 00039: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3143 - accuracy: 0.8889 - val_loss: 0.4614 - val_accuracy: 0.8488\n",
      "Epoch 40/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8915\n",
      "Epoch 00040: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3066 - accuracy: 0.8915 - val_loss: 0.4644 - val_accuracy: 0.8485\n",
      "Epoch 41/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8924\n",
      "Epoch 00041: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.3067 - accuracy: 0.8924 - val_loss: 0.4824 - val_accuracy: 0.8399\n",
      "Epoch 42/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8957\n",
      "Epoch 00042: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2968 - accuracy: 0.8957 - val_loss: 0.4562 - val_accuracy: 0.8477\n",
      "Epoch 43/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8979\n",
      "Epoch 00043: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.2908 - accuracy: 0.8979 - val_loss: 0.4554 - val_accuracy: 0.8487\n",
      "Epoch 44/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8978\n",
      "Epoch 00044: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2916 - accuracy: 0.8979 - val_loss: 0.5087 - val_accuracy: 0.8341\n",
      "Epoch 45/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2816 - accuracy: 0.9006\n",
      "Epoch 00045: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2814 - accuracy: 0.9006 - val_loss: 0.5183 - val_accuracy: 0.8372\n",
      "Epoch 46/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.9001\n",
      "Epoch 00046: val_accuracy did not improve from 0.85296\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2832 - accuracy: 0.9000 - val_loss: 0.5110 - val_accuracy: 0.8396\n",
      "Epoch 47/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.9013\n",
      "Epoch 00047: val_accuracy improved from 0.85296 to 0.85367, saving model to training_1/cp-0047.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.2791 - accuracy: 0.9013 - val_loss: 0.4435 - val_accuracy: 0.8537\n",
      "Epoch 48/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.9040\n",
      "Epoch 00048: val_accuracy did not improve from 0.85367\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2731 - accuracy: 0.9040 - val_loss: 0.4814 - val_accuracy: 0.8498\n",
      "Epoch 49/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.9033\n",
      "Epoch 00049: val_accuracy improved from 0.85367 to 0.86338, saving model to training_1/cp-0049.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.2774 - accuracy: 0.9033 - val_loss: 0.4121 - val_accuracy: 0.8634\n",
      "Epoch 50/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.9064\n",
      "Epoch 00050: val_accuracy improved from 0.86338 to 0.86859, saving model to training_1/cp-0050.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.2677 - accuracy: 0.9065 - val_loss: 0.4039 - val_accuracy: 0.8686\n",
      "Epoch 51/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.9061\n",
      "Epoch 00051: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.2671 - accuracy: 0.9061 - val_loss: 0.3929 - val_accuracy: 0.8663\n",
      "Epoch 52/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2593 - accuracy: 0.9079\n",
      "Epoch 00052: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.2594 - accuracy: 0.9079 - val_loss: 0.4801 - val_accuracy: 0.8447\n",
      "Epoch 53/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2632 - accuracy: 0.9080\n",
      "Epoch 00053: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2632 - accuracy: 0.9080 - val_loss: 0.4849 - val_accuracy: 0.8432\n",
      "Epoch 54/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.9111\n",
      "Epoch 00054: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2495 - accuracy: 0.9112 - val_loss: 0.4184 - val_accuracy: 0.8631\n",
      "Epoch 55/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9109\n",
      "Epoch 00055: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2537 - accuracy: 0.9109 - val_loss: 0.5016 - val_accuracy: 0.8387\n",
      "Epoch 56/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9124\n",
      "Epoch 00056: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2495 - accuracy: 0.9123 - val_loss: 0.4505 - val_accuracy: 0.8507\n",
      "Epoch 57/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9130\n",
      "Epoch 00057: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2444 - accuracy: 0.9131 - val_loss: 0.4525 - val_accuracy: 0.8576\n",
      "Epoch 58/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9132\n",
      "Epoch 00058: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2440 - accuracy: 0.9131 - val_loss: 0.4157 - val_accuracy: 0.8635\n",
      "Epoch 59/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9138\n",
      "Epoch 00059: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2435 - accuracy: 0.9138 - val_loss: 0.4295 - val_accuracy: 0.8583\n",
      "Epoch 60/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9153\n",
      "Epoch 00060: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2390 - accuracy: 0.9154 - val_loss: 0.4235 - val_accuracy: 0.8629\n",
      "Epoch 61/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9162\n",
      "Epoch 00061: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2356 - accuracy: 0.9163 - val_loss: 0.4034 - val_accuracy: 0.8673\n",
      "Epoch 62/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9180\n",
      "Epoch 00062: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2318 - accuracy: 0.9180 - val_loss: 0.4157 - val_accuracy: 0.8674\n",
      "Epoch 63/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2289 - accuracy: 0.9195\n",
      "Epoch 00063: val_accuracy did not improve from 0.86859\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2288 - accuracy: 0.9195 - val_loss: 0.4733 - val_accuracy: 0.8528\n",
      "Epoch 64/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9187\n",
      "Epoch 00064: val_accuracy improved from 0.86859 to 0.87029, saving model to training_1/cp-0064.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.2286 - accuracy: 0.9186 - val_loss: 0.3978 - val_accuracy: 0.8703\n",
      "Epoch 65/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9209\n",
      "Epoch 00065: val_accuracy did not improve from 0.87029\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2273 - accuracy: 0.9208 - val_loss: 0.4055 - val_accuracy: 0.8700\n",
      "Epoch 66/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9213\n",
      "Epoch 00066: val_accuracy improved from 0.87029 to 0.87430, saving model to training_1/cp-0066.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.2221 - accuracy: 0.9212 - val_loss: 0.3877 - val_accuracy: 0.8743\n",
      "Epoch 67/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9227\n",
      "Epoch 00067: val_accuracy did not improve from 0.87430\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2190 - accuracy: 0.9227 - val_loss: 0.4105 - val_accuracy: 0.8688\n",
      "Epoch 68/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9234\n",
      "Epoch 00068: val_accuracy did not improve from 0.87430\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2179 - accuracy: 0.9234 - val_loss: 0.3986 - val_accuracy: 0.8703\n",
      "Epoch 69/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2136 - accuracy: 0.9241\n",
      "Epoch 00069: val_accuracy did not improve from 0.87430\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.2136 - accuracy: 0.9242 - val_loss: 0.4146 - val_accuracy: 0.8716\n",
      "Epoch 70/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9234\n",
      "Epoch 00070: val_accuracy improved from 0.87430 to 0.87660, saving model to training_1/cp-0070.ckpt\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.2145 - accuracy: 0.9234 - val_loss: 0.4110 - val_accuracy: 0.8766\n",
      "Epoch 71/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2160 - accuracy: 0.9229\n",
      "Epoch 00071: val_accuracy did not improve from 0.87660\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.2160 - accuracy: 0.9229 - val_loss: 0.4076 - val_accuracy: 0.8650\n",
      "Epoch 72/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9253\n",
      "Epoch 00072: val_accuracy did not improve from 0.87660\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2111 - accuracy: 0.9254 - val_loss: 0.3913 - val_accuracy: 0.8752\n",
      "Epoch 73/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2103 - accuracy: 0.9255\n",
      "Epoch 00073: val_accuracy did not improve from 0.87660\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2103 - accuracy: 0.9255 - val_loss: 0.5194 - val_accuracy: 0.8470\n",
      "Epoch 74/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9255\n",
      "Epoch 00074: val_accuracy improved from 0.87660 to 0.87730, saving model to training_1/cp-0074.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2087 - accuracy: 0.9255 - val_loss: 0.3996 - val_accuracy: 0.8773\n",
      "Epoch 75/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9287\n",
      "Epoch 00075: val_accuracy did not improve from 0.87730\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.2036 - accuracy: 0.9288 - val_loss: 0.3929 - val_accuracy: 0.8762\n",
      "Epoch 76/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9285\n",
      "Epoch 00076: val_accuracy did not improve from 0.87730\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2025 - accuracy: 0.9285 - val_loss: 0.3924 - val_accuracy: 0.8758\n",
      "Epoch 77/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9291\n",
      "Epoch 00077: val_accuracy did not improve from 0.87730\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2003 - accuracy: 0.9291 - val_loss: 0.4013 - val_accuracy: 0.8702\n",
      "Epoch 78/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9261\n",
      "Epoch 00078: val_accuracy did not improve from 0.87730\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.2078 - accuracy: 0.9261 - val_loss: 0.4031 - val_accuracy: 0.8733\n",
      "Epoch 79/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1959 - accuracy: 0.9305\n",
      "Epoch 00079: val_accuracy did not improve from 0.87730\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1959 - accuracy: 0.9304 - val_loss: 0.4320 - val_accuracy: 0.8655\n",
      "Epoch 80/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9316\n",
      "Epoch 00080: val_accuracy improved from 0.87730 to 0.88001, saving model to training_1/cp-0080.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1948 - accuracy: 0.9316 - val_loss: 0.3748 - val_accuracy: 0.8800\n",
      "Epoch 81/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9302\n",
      "Epoch 00081: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1964 - accuracy: 0.9302 - val_loss: 0.4047 - val_accuracy: 0.8699\n",
      "Epoch 82/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9312\n",
      "Epoch 00082: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1948 - accuracy: 0.9313 - val_loss: 0.4712 - val_accuracy: 0.8609\n",
      "Epoch 83/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9323\n",
      "Epoch 00083: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1919 - accuracy: 0.9323 - val_loss: 0.4024 - val_accuracy: 0.8746\n",
      "Epoch 84/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9324\n",
      "Epoch 00084: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1893 - accuracy: 0.9324 - val_loss: 0.4057 - val_accuracy: 0.8708\n",
      "Epoch 85/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9327\n",
      "Epoch 00085: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1871 - accuracy: 0.9327 - val_loss: 0.4262 - val_accuracy: 0.8682\n",
      "Epoch 86/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9340\n",
      "Epoch 00086: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1863 - accuracy: 0.9339 - val_loss: 0.4312 - val_accuracy: 0.8671\n",
      "Epoch 87/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9329\n",
      "Epoch 00087: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1867 - accuracy: 0.9329 - val_loss: 0.4009 - val_accuracy: 0.8763\n",
      "Epoch 88/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9335\n",
      "Epoch 00088: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1874 - accuracy: 0.9334 - val_loss: 0.4205 - val_accuracy: 0.8748\n",
      "Epoch 89/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9357\n",
      "Epoch 00089: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1832 - accuracy: 0.9357 - val_loss: 0.4213 - val_accuracy: 0.8670\n",
      "Epoch 90/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9361\n",
      "Epoch 00090: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1816 - accuracy: 0.9360 - val_loss: 0.4115 - val_accuracy: 0.8730\n",
      "Epoch 91/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9361\n",
      "Epoch 00091: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1790 - accuracy: 0.9361 - val_loss: 0.4367 - val_accuracy: 0.8732\n",
      "Epoch 92/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9358\n",
      "Epoch 00092: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1783 - accuracy: 0.9358 - val_loss: 0.3915 - val_accuracy: 0.8773\n",
      "Epoch 93/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9364\n",
      "Epoch 00093: val_accuracy did not improve from 0.88001\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1782 - accuracy: 0.9365 - val_loss: 0.4152 - val_accuracy: 0.8763\n",
      "Epoch 94/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9391\n",
      "Epoch 00094: val_accuracy improved from 0.88001 to 0.88452, saving model to training_1/cp-0094.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1731 - accuracy: 0.9390 - val_loss: 0.3714 - val_accuracy: 0.8845\n",
      "Epoch 95/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9380\n",
      "Epoch 00095: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1759 - accuracy: 0.9380 - val_loss: 0.3993 - val_accuracy: 0.8784\n",
      "Epoch 96/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9388\n",
      "Epoch 00096: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1723 - accuracy: 0.9388 - val_loss: 0.4082 - val_accuracy: 0.8769\n",
      "Epoch 97/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9380\n",
      "Epoch 00097: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1732 - accuracy: 0.9380 - val_loss: 0.3975 - val_accuracy: 0.8801\n",
      "Epoch 98/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9394\n",
      "Epoch 00098: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1710 - accuracy: 0.9394 - val_loss: 0.4157 - val_accuracy: 0.8776\n",
      "Epoch 99/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9397\n",
      "Epoch 00099: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1683 - accuracy: 0.9397 - val_loss: 0.4235 - val_accuracy: 0.8755\n",
      "Epoch 100/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9406\n",
      "Epoch 00100: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1707 - accuracy: 0.9406 - val_loss: 0.4603 - val_accuracy: 0.8635\n",
      "Epoch 101/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9402\n",
      "Epoch 00101: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1688 - accuracy: 0.9402 - val_loss: 0.3923 - val_accuracy: 0.8765\n",
      "Epoch 102/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9391\n",
      "Epoch 00102: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1691 - accuracy: 0.9390 - val_loss: 0.4049 - val_accuracy: 0.8812\n",
      "Epoch 103/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9414\n",
      "Epoch 00103: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1630 - accuracy: 0.9415 - val_loss: 0.3950 - val_accuracy: 0.8790\n",
      "Epoch 104/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1620 - accuracy: 0.9418\n",
      "Epoch 00104: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1620 - accuracy: 0.9418 - val_loss: 0.3791 - val_accuracy: 0.8833\n",
      "Epoch 105/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9416\n",
      "Epoch 00105: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1639 - accuracy: 0.9416 - val_loss: 0.4511 - val_accuracy: 0.8682\n",
      "Epoch 106/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9425\n",
      "Epoch 00106: val_accuracy did not improve from 0.88452\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1639 - accuracy: 0.9425 - val_loss: 0.3873 - val_accuracy: 0.8820\n",
      "Epoch 107/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9436\n",
      "Epoch 00107: val_accuracy improved from 0.88452 to 0.88462, saving model to training_1/cp-0107.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1585 - accuracy: 0.9436 - val_loss: 0.3922 - val_accuracy: 0.8846\n",
      "Epoch 108/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9444\n",
      "Epoch 00108: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1562 - accuracy: 0.9444 - val_loss: 0.3873 - val_accuracy: 0.8811\n",
      "Epoch 109/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1610 - accuracy: 0.9432\n",
      "Epoch 00109: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1611 - accuracy: 0.9432 - val_loss: 0.4242 - val_accuracy: 0.8756\n",
      "Epoch 110/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9452\n",
      "Epoch 00110: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1563 - accuracy: 0.9452 - val_loss: 0.4312 - val_accuracy: 0.8729\n",
      "Epoch 111/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9445\n",
      "Epoch 00111: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1553 - accuracy: 0.9446 - val_loss: 0.4211 - val_accuracy: 0.8756\n",
      "Epoch 112/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9461\n",
      "Epoch 00112: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1547 - accuracy: 0.9461 - val_loss: 0.3970 - val_accuracy: 0.8823\n",
      "Epoch 113/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9465\n",
      "Epoch 00113: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1534 - accuracy: 0.9465 - val_loss: 0.4081 - val_accuracy: 0.8775\n",
      "Epoch 114/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9461\n",
      "Epoch 00114: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1524 - accuracy: 0.9462 - val_loss: 0.3959 - val_accuracy: 0.8790\n",
      "Epoch 115/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9453\n",
      "Epoch 00115: val_accuracy did not improve from 0.88462\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1508 - accuracy: 0.9453 - val_loss: 0.4012 - val_accuracy: 0.8796\n",
      "Epoch 116/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.9459\n",
      "Epoch 00116: val_accuracy improved from 0.88462 to 0.88622, saving model to training_1/cp-0116.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1520 - accuracy: 0.9459 - val_loss: 0.3807 - val_accuracy: 0.8862\n",
      "Epoch 117/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9480\n",
      "Epoch 00117: val_accuracy did not improve from 0.88622\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1475 - accuracy: 0.9479 - val_loss: 0.4194 - val_accuracy: 0.8773\n",
      "Epoch 118/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9488\n",
      "Epoch 00118: val_accuracy did not improve from 0.88622\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1464 - accuracy: 0.9488 - val_loss: 0.4120 - val_accuracy: 0.8823\n",
      "Epoch 119/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9464\n",
      "Epoch 00119: val_accuracy improved from 0.88622 to 0.88972, saving model to training_1/cp-0119.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1509 - accuracy: 0.9464 - val_loss: 0.3652 - val_accuracy: 0.8897\n",
      "Epoch 120/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9476\n",
      "Epoch 00120: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1447 - accuracy: 0.9476 - val_loss: 0.4359 - val_accuracy: 0.8773\n",
      "Epoch 121/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9454\n",
      "Epoch 00121: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1498 - accuracy: 0.9454 - val_loss: 0.3991 - val_accuracy: 0.8828\n",
      "Epoch 122/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9493\n",
      "Epoch 00122: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1461 - accuracy: 0.9493 - val_loss: 0.3812 - val_accuracy: 0.8862\n",
      "Epoch 123/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9480\n",
      "Epoch 00123: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1487 - accuracy: 0.9480 - val_loss: 0.3896 - val_accuracy: 0.8856\n",
      "Epoch 124/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9475\n",
      "Epoch 00124: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1466 - accuracy: 0.9475 - val_loss: 0.4205 - val_accuracy: 0.8783\n",
      "Epoch 125/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9489\n",
      "Epoch 00125: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1430 - accuracy: 0.9489 - val_loss: 0.4045 - val_accuracy: 0.8799\n",
      "Epoch 126/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9488\n",
      "Epoch 00126: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1447 - accuracy: 0.9488 - val_loss: 0.3880 - val_accuracy: 0.8856\n",
      "Epoch 127/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9504\n",
      "Epoch 00127: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1413 - accuracy: 0.9503 - val_loss: 0.3931 - val_accuracy: 0.8890\n",
      "Epoch 128/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9509\n",
      "Epoch 00128: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1392 - accuracy: 0.9509 - val_loss: 0.3871 - val_accuracy: 0.8846\n",
      "Epoch 129/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9498\n",
      "Epoch 00129: val_accuracy did not improve from 0.88972\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1410 - accuracy: 0.9498 - val_loss: 0.4066 - val_accuracy: 0.8824\n",
      "Epoch 130/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9505\n",
      "Epoch 00130: val_accuracy improved from 0.88972 to 0.89143, saving model to training_1/cp-0130.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1388 - accuracy: 0.9505 - val_loss: 0.3711 - val_accuracy: 0.8914\n",
      "Epoch 131/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9510\n",
      "Epoch 00131: val_accuracy did not improve from 0.89143\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1389 - accuracy: 0.9510 - val_loss: 0.3891 - val_accuracy: 0.8841\n",
      "Epoch 132/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9508\n",
      "Epoch 00132: val_accuracy did not improve from 0.89143\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1380 - accuracy: 0.9507 - val_loss: 0.4244 - val_accuracy: 0.8773\n",
      "Epoch 133/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9503\n",
      "Epoch 00133: val_accuracy did not improve from 0.89143\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1394 - accuracy: 0.9503 - val_loss: 0.4115 - val_accuracy: 0.8806\n",
      "Epoch 134/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9520\n",
      "Epoch 00134: val_accuracy did not improve from 0.89143\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1359 - accuracy: 0.9519 - val_loss: 0.4107 - val_accuracy: 0.8824\n",
      "Epoch 135/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9519\n",
      "Epoch 00135: val_accuracy did not improve from 0.89143\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1378 - accuracy: 0.9519 - val_loss: 0.4580 - val_accuracy: 0.8673\n",
      "Epoch 136/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9519\n",
      "Epoch 00136: val_accuracy improved from 0.89143 to 0.89233, saving model to training_1/cp-0136.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1326 - accuracy: 0.9519 - val_loss: 0.3755 - val_accuracy: 0.8923\n",
      "Epoch 137/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9523\n",
      "Epoch 00137: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1338 - accuracy: 0.9523 - val_loss: 0.3717 - val_accuracy: 0.8916\n",
      "Epoch 138/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9524\n",
      "Epoch 00138: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1335 - accuracy: 0.9524 - val_loss: 0.3804 - val_accuracy: 0.8846\n",
      "Epoch 139/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9542\n",
      "Epoch 00139: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1314 - accuracy: 0.9542 - val_loss: 0.3938 - val_accuracy: 0.8877\n",
      "Epoch 140/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9528\n",
      "Epoch 00140: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1340 - accuracy: 0.9528 - val_loss: 0.3771 - val_accuracy: 0.8918\n",
      "Epoch 141/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9548\n",
      "Epoch 00141: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1298 - accuracy: 0.9548 - val_loss: 0.4197 - val_accuracy: 0.8814\n",
      "Epoch 142/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9525\n",
      "Epoch 00142: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1327 - accuracy: 0.9525 - val_loss: 0.3853 - val_accuracy: 0.8849\n",
      "Epoch 143/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9546\n",
      "Epoch 00143: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1285 - accuracy: 0.9546 - val_loss: 0.4024 - val_accuracy: 0.8832\n",
      "Epoch 144/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9538\n",
      "Epoch 00144: val_accuracy did not improve from 0.89233\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1296 - accuracy: 0.9538 - val_loss: 0.3896 - val_accuracy: 0.8866\n",
      "Epoch 145/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9550\n",
      "Epoch 00145: val_accuracy improved from 0.89233 to 0.89353, saving model to training_1/cp-0145.ckpt\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1296 - accuracy: 0.9550 - val_loss: 0.3843 - val_accuracy: 0.8935\n",
      "Epoch 146/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9542\n",
      "Epoch 00146: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1278 - accuracy: 0.9541 - val_loss: 0.4393 - val_accuracy: 0.8774\n",
      "Epoch 147/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9546\n",
      "Epoch 00147: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1282 - accuracy: 0.9546 - val_loss: 0.4267 - val_accuracy: 0.8806\n",
      "Epoch 148/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9551\n",
      "Epoch 00148: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1264 - accuracy: 0.9550 - val_loss: 0.4018 - val_accuracy: 0.8862\n",
      "Epoch 149/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9569\n",
      "Epoch 00149: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1241 - accuracy: 0.9570 - val_loss: 0.4123 - val_accuracy: 0.8818\n",
      "Epoch 150/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9564\n",
      "Epoch 00150: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1231 - accuracy: 0.9564 - val_loss: 0.4241 - val_accuracy: 0.8799\n",
      "Epoch 151/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9549\n",
      "Epoch 00151: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1234 - accuracy: 0.9549 - val_loss: 0.5034 - val_accuracy: 0.8629\n",
      "Epoch 152/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9570\n",
      "Epoch 00152: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1211 - accuracy: 0.9570 - val_loss: 0.4459 - val_accuracy: 0.8795\n",
      "Epoch 153/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9567\n",
      "Epoch 00153: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1239 - accuracy: 0.9568 - val_loss: 0.4202 - val_accuracy: 0.8813\n",
      "Epoch 154/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9557\n",
      "Epoch 00154: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1255 - accuracy: 0.9557 - val_loss: 0.4019 - val_accuracy: 0.8835\n",
      "Epoch 155/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9564\n",
      "Epoch 00155: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1226 - accuracy: 0.9565 - val_loss: 0.3986 - val_accuracy: 0.8884\n",
      "Epoch 156/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9584\n",
      "Epoch 00156: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1168 - accuracy: 0.9583 - val_loss: 0.4363 - val_accuracy: 0.8805\n",
      "Epoch 157/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9572\n",
      "Epoch 00157: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1210 - accuracy: 0.9572 - val_loss: 0.4210 - val_accuracy: 0.8840\n",
      "Epoch 158/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9581\n",
      "Epoch 00158: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1192 - accuracy: 0.9581 - val_loss: 0.4135 - val_accuracy: 0.8836\n",
      "Epoch 159/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9571\n",
      "Epoch 00159: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1200 - accuracy: 0.9572 - val_loss: 0.4069 - val_accuracy: 0.8822\n",
      "Epoch 160/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9570\n",
      "Epoch 00160: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1207 - accuracy: 0.9570 - val_loss: 0.4097 - val_accuracy: 0.8905\n",
      "Epoch 161/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9584\n",
      "Epoch 00161: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1169 - accuracy: 0.9584 - val_loss: 0.3989 - val_accuracy: 0.8852\n",
      "Epoch 162/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9585\n",
      "Epoch 00162: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1165 - accuracy: 0.9585 - val_loss: 0.3637 - val_accuracy: 0.8923\n",
      "Epoch 163/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9578\n",
      "Epoch 00163: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1192 - accuracy: 0.9578 - val_loss: 0.3896 - val_accuracy: 0.8903\n",
      "Epoch 164/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9586\n",
      "Epoch 00164: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1167 - accuracy: 0.9587 - val_loss: 0.4071 - val_accuracy: 0.8849\n",
      "Epoch 165/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9574\n",
      "Epoch 00165: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1193 - accuracy: 0.9574 - val_loss: 0.4000 - val_accuracy: 0.8872\n",
      "Epoch 166/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9589\n",
      "Epoch 00166: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1168 - accuracy: 0.9589 - val_loss: 0.4075 - val_accuracy: 0.8872\n",
      "Epoch 167/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9583\n",
      "Epoch 00167: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1177 - accuracy: 0.9583 - val_loss: 0.4488 - val_accuracy: 0.8786\n",
      "Epoch 168/200\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9579\n",
      "Epoch 00168: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1171 - accuracy: 0.9580 - val_loss: 0.3921 - val_accuracy: 0.8913\n",
      "Epoch 169/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9588\n",
      "Epoch 00169: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1172 - accuracy: 0.9588 - val_loss: 0.4111 - val_accuracy: 0.8811\n",
      "Epoch 170/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9594\n",
      "Epoch 00170: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1148 - accuracy: 0.9594 - val_loss: 0.3992 - val_accuracy: 0.8899\n",
      "Epoch 171/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9592\n",
      "Epoch 00171: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1140 - accuracy: 0.9592 - val_loss: 0.4112 - val_accuracy: 0.8899\n",
      "Epoch 172/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9591\n",
      "Epoch 00172: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1143 - accuracy: 0.9592 - val_loss: 0.4401 - val_accuracy: 0.8802\n",
      "Epoch 173/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9586\n",
      "Epoch 00173: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1178 - accuracy: 0.9586 - val_loss: 0.4054 - val_accuracy: 0.8855\n",
      "Epoch 174/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9610\n",
      "Epoch 00174: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1109 - accuracy: 0.9610 - val_loss: 0.4019 - val_accuracy: 0.8893\n",
      "Epoch 175/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9612\n",
      "Epoch 00175: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1097 - accuracy: 0.9612 - val_loss: 0.3987 - val_accuracy: 0.8892\n",
      "Epoch 176/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9605\n",
      "Epoch 00176: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1115 - accuracy: 0.9606 - val_loss: 0.4234 - val_accuracy: 0.8863\n",
      "Epoch 177/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9609\n",
      "Epoch 00177: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1111 - accuracy: 0.9609 - val_loss: 0.4005 - val_accuracy: 0.8895\n",
      "Epoch 178/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9610\n",
      "Epoch 00178: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1121 - accuracy: 0.9610 - val_loss: 0.4162 - val_accuracy: 0.8845\n",
      "Epoch 179/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9619\n",
      "Epoch 00179: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1073 - accuracy: 0.9619 - val_loss: 0.4479 - val_accuracy: 0.8816\n",
      "Epoch 180/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9603\n",
      "Epoch 00180: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1084 - accuracy: 0.9603 - val_loss: 0.4405 - val_accuracy: 0.8799\n",
      "Epoch 181/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9623\n",
      "Epoch 00181: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1085 - accuracy: 0.9623 - val_loss: 0.4052 - val_accuracy: 0.8870\n",
      "Epoch 182/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9615\n",
      "Epoch 00182: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1090 - accuracy: 0.9614 - val_loss: 0.3920 - val_accuracy: 0.8888\n",
      "Epoch 183/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9629\n",
      "Epoch 00183: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1060 - accuracy: 0.9629 - val_loss: 0.4265 - val_accuracy: 0.8876\n",
      "Epoch 184/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9613\n",
      "Epoch 00184: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1102 - accuracy: 0.9613 - val_loss: 0.3989 - val_accuracy: 0.8895\n",
      "Epoch 185/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9621\n",
      "Epoch 00185: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1091 - accuracy: 0.9622 - val_loss: 0.4366 - val_accuracy: 0.8858\n",
      "Epoch 186/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9622\n",
      "Epoch 00186: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1077 - accuracy: 0.9622 - val_loss: 0.4037 - val_accuracy: 0.8890\n",
      "Epoch 187/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9630\n",
      "Epoch 00187: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1070 - accuracy: 0.9629 - val_loss: 0.4521 - val_accuracy: 0.8825\n",
      "Epoch 188/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9644\n",
      "Epoch 00188: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1032 - accuracy: 0.9643 - val_loss: 0.4531 - val_accuracy: 0.8779\n",
      "Epoch 189/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9630\n",
      "Epoch 00189: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.1053 - accuracy: 0.9630 - val_loss: 0.4220 - val_accuracy: 0.8879\n",
      "Epoch 190/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9632\n",
      "Epoch 00190: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1035 - accuracy: 0.9631 - val_loss: 0.3830 - val_accuracy: 0.8922\n",
      "Epoch 191/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9623\n",
      "Epoch 00191: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.1082 - accuracy: 0.9623 - val_loss: 0.4103 - val_accuracy: 0.8891\n",
      "Epoch 192/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9632\n",
      "Epoch 00192: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1043 - accuracy: 0.9632 - val_loss: 0.3973 - val_accuracy: 0.8901\n",
      "Epoch 193/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9632\n",
      "Epoch 00193: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1033 - accuracy: 0.9632 - val_loss: 0.4204 - val_accuracy: 0.8864\n",
      "Epoch 194/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9628\n",
      "Epoch 00194: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1056 - accuracy: 0.9628 - val_loss: 0.4354 - val_accuracy: 0.8800\n",
      "Epoch 195/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9644\n",
      "Epoch 00195: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1026 - accuracy: 0.9644 - val_loss: 0.4597 - val_accuracy: 0.8811\n",
      "Epoch 196/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9642\n",
      "Epoch 00196: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1022 - accuracy: 0.9642 - val_loss: 0.4062 - val_accuracy: 0.8918\n",
      "Epoch 197/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9636\n",
      "Epoch 00197: val_accuracy did not improve from 0.89353\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1025 - accuracy: 0.9636 - val_loss: 0.3967 - val_accuracy: 0.8913\n",
      "Epoch 198/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9654\n",
      "Epoch 00198: val_accuracy improved from 0.89353 to 0.89694, saving model to training_1/cp-0198.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1002 - accuracy: 0.9654 - val_loss: 0.3697 - val_accuracy: 0.8969\n",
      "Epoch 199/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9640\n",
      "Epoch 00199: val_accuracy did not improve from 0.89694\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1017 - accuracy: 0.9641 - val_loss: 0.4056 - val_accuracy: 0.8917\n",
      "Epoch 200/200\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9631\n",
      "Epoch 00200: val_accuracy did not improve from 0.89694\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1042 - accuracy: 0.9631 - val_loss: 0.3639 - val_accuracy: 0.8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1783f19b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "import os\n",
    "checkpoint_path=\"training_1/cp-{epoch:04d}.ckpt\" \n",
    "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1,save_weights_only=True)\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=(len(X_train) // 32),\n",
    "        epochs=200,\n",
    "        validation_data=test_generator,callbacks=[checkpoint],\n",
    "        validation_steps=(len(X_test) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EjdtKI-V9NHY",
    "outputId": "51faee66-ee24-405b-8087-e9c77ef3e57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1562 steps, validate for 312 steps\n",
      "Epoch 1/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9657\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89473, saving model to training_2/cp-0001.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0981 - accuracy: 0.9657 - val_loss: 0.4063 - val_accuracy: 0.8947\n",
      "Epoch 2/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9654\n",
      "Epoch 00002: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0997 - accuracy: 0.9654 - val_loss: 0.4406 - val_accuracy: 0.8847\n",
      "Epoch 3/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9641\n",
      "Epoch 00003: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.1025 - accuracy: 0.9641 - val_loss: 0.4094 - val_accuracy: 0.8889\n",
      "Epoch 4/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9658\n",
      "Epoch 00004: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0967 - accuracy: 0.9658 - val_loss: 0.4588 - val_accuracy: 0.8822\n",
      "Epoch 5/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9651\n",
      "Epoch 00005: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0991 - accuracy: 0.9651 - val_loss: 0.3941 - val_accuracy: 0.8935\n",
      "Epoch 6/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9654\n",
      "Epoch 00006: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0999 - accuracy: 0.9654 - val_loss: 0.4256 - val_accuracy: 0.8865\n",
      "Epoch 7/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9645\n",
      "Epoch 00007: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.0990 - accuracy: 0.9645 - val_loss: 0.4073 - val_accuracy: 0.8908\n",
      "Epoch 8/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9659\n",
      "Epoch 00008: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 92s 59ms/step - loss: 0.0980 - accuracy: 0.9659 - val_loss: 0.4372 - val_accuracy: 0.8844\n",
      "Epoch 9/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9661\n",
      "Epoch 00009: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 91s 59ms/step - loss: 0.0970 - accuracy: 0.9661 - val_loss: 0.4034 - val_accuracy: 0.8912\n",
      "Epoch 10/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9659\n",
      "Epoch 00010: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.0963 - accuracy: 0.9659 - val_loss: 0.4242 - val_accuracy: 0.8865\n",
      "Epoch 11/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9642\n",
      "Epoch 00011: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.1009 - accuracy: 0.9642 - val_loss: 0.4076 - val_accuracy: 0.8913\n",
      "Epoch 12/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9661\n",
      "Epoch 00012: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0948 - accuracy: 0.9661 - val_loss: 0.4015 - val_accuracy: 0.8945\n",
      "Epoch 13/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9655\n",
      "Epoch 00013: val_accuracy did not improve from 0.89473\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0963 - accuracy: 0.9656 - val_loss: 0.4453 - val_accuracy: 0.8785\n",
      "Epoch 14/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9661\n",
      "Epoch 00014: val_accuracy improved from 0.89473 to 0.89613, saving model to training_2/cp-0014.ckpt\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0973 - accuracy: 0.9660 - val_loss: 0.3914 - val_accuracy: 0.8961\n",
      "Epoch 15/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9660\n",
      "Epoch 00015: val_accuracy did not improve from 0.89613\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0975 - accuracy: 0.9660 - val_loss: 0.4105 - val_accuracy: 0.8897\n",
      "Epoch 16/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9665\n",
      "Epoch 00016: val_accuracy did not improve from 0.89613\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0944 - accuracy: 0.9665 - val_loss: 0.3912 - val_accuracy: 0.8885\n",
      "Epoch 17/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9672\n",
      "Epoch 00017: val_accuracy improved from 0.89613 to 0.89884, saving model to training_2/cp-0017.ckpt\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0927 - accuracy: 0.9672 - val_loss: 0.3843 - val_accuracy: 0.8988\n",
      "Epoch 18/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9678\n",
      "Epoch 00018: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0918 - accuracy: 0.9678 - val_loss: 0.3869 - val_accuracy: 0.8966\n",
      "Epoch 19/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9662\n",
      "Epoch 00019: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0982 - accuracy: 0.9661 - val_loss: 0.3722 - val_accuracy: 0.8978\n",
      "Epoch 20/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9673\n",
      "Epoch 00020: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0934 - accuracy: 0.9673 - val_loss: 0.3855 - val_accuracy: 0.8899\n",
      "Epoch 21/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9663\n",
      "Epoch 00021: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0955 - accuracy: 0.9663 - val_loss: 0.4066 - val_accuracy: 0.8893\n",
      "Epoch 22/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9665\n",
      "Epoch 00022: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0936 - accuracy: 0.9665 - val_loss: 0.4007 - val_accuracy: 0.8907\n",
      "Epoch 23/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9675\n",
      "Epoch 00023: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0918 - accuracy: 0.9675 - val_loss: 0.4324 - val_accuracy: 0.8869\n",
      "Epoch 24/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9676\n",
      "Epoch 00024: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0914 - accuracy: 0.9675 - val_loss: 0.3936 - val_accuracy: 0.8940\n",
      "Epoch 25/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9666\n",
      "Epoch 00025: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0928 - accuracy: 0.9666 - val_loss: 0.4014 - val_accuracy: 0.8954\n",
      "Epoch 26/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9671\n",
      "Epoch 00026: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0921 - accuracy: 0.9670 - val_loss: 0.3845 - val_accuracy: 0.8939\n",
      "Epoch 27/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9671\n",
      "Epoch 00027: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 91s 58ms/step - loss: 0.0931 - accuracy: 0.9671 - val_loss: 0.4133 - val_accuracy: 0.8931\n",
      "Epoch 28/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9682\n",
      "Epoch 00028: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0908 - accuracy: 0.9682 - val_loss: 0.4102 - val_accuracy: 0.8919\n",
      "Epoch 29/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9682\n",
      "Epoch 00029: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 90s 57ms/step - loss: 0.0911 - accuracy: 0.9682 - val_loss: 0.3996 - val_accuracy: 0.8939\n",
      "Epoch 30/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9689\n",
      "Epoch 00030: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0893 - accuracy: 0.9689 - val_loss: 0.4083 - val_accuracy: 0.8896\n",
      "Epoch 31/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9674\n",
      "Epoch 00031: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0931 - accuracy: 0.9674 - val_loss: 0.5035 - val_accuracy: 0.8761\n",
      "Epoch 32/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9681\n",
      "Epoch 00032: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0926 - accuracy: 0.9681 - val_loss: 0.4176 - val_accuracy: 0.8879\n",
      "Epoch 33/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9678\n",
      "Epoch 00033: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0929 - accuracy: 0.9678 - val_loss: 0.4033 - val_accuracy: 0.8934\n",
      "Epoch 34/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9674\n",
      "Epoch 00034: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0906 - accuracy: 0.9674 - val_loss: 0.3878 - val_accuracy: 0.8968\n",
      "Epoch 35/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9675\n",
      "Epoch 00035: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0899 - accuracy: 0.9676 - val_loss: 0.3805 - val_accuracy: 0.8982\n",
      "Epoch 36/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9682\n",
      "Epoch 00036: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0904 - accuracy: 0.9682 - val_loss: 0.3997 - val_accuracy: 0.8950\n",
      "Epoch 37/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9678\n",
      "Epoch 00037: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0904 - accuracy: 0.9678 - val_loss: 0.4094 - val_accuracy: 0.8879\n",
      "Epoch 38/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9691\n",
      "Epoch 00038: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0906 - accuracy: 0.9692 - val_loss: 0.3941 - val_accuracy: 0.8965\n",
      "Epoch 39/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9689\n",
      "Epoch 00039: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0877 - accuracy: 0.9688 - val_loss: 0.4356 - val_accuracy: 0.8919\n",
      "Epoch 40/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9690\n",
      "Epoch 00040: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0877 - accuracy: 0.9690 - val_loss: 0.4204 - val_accuracy: 0.8923\n",
      "Epoch 41/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9688\n",
      "Epoch 00041: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0892 - accuracy: 0.9687 - val_loss: 0.3938 - val_accuracy: 0.8945\n",
      "Epoch 42/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9687\n",
      "Epoch 00042: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0877 - accuracy: 0.9687 - val_loss: 0.4291 - val_accuracy: 0.8859\n",
      "Epoch 43/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9705\n",
      "Epoch 00043: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0851 - accuracy: 0.9705 - val_loss: 0.4166 - val_accuracy: 0.8886\n",
      "Epoch 44/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9684\n",
      "Epoch 00044: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0892 - accuracy: 0.9685 - val_loss: 0.4229 - val_accuracy: 0.8873\n",
      "Epoch 45/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9688\n",
      "Epoch 00045: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0882 - accuracy: 0.9688 - val_loss: 0.4418 - val_accuracy: 0.8837\n",
      "Epoch 46/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9691\n",
      "Epoch 00046: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0869 - accuracy: 0.9691 - val_loss: 0.4414 - val_accuracy: 0.8855\n",
      "Epoch 47/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9694\n",
      "Epoch 00047: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 57ms/step - loss: 0.0866 - accuracy: 0.9694 - val_loss: 0.3996 - val_accuracy: 0.8915\n",
      "Epoch 48/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9691\n",
      "Epoch 00048: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0877 - accuracy: 0.9692 - val_loss: 0.3888 - val_accuracy: 0.8949\n",
      "Epoch 49/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9710\n",
      "Epoch 00049: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 90s 58ms/step - loss: 0.0845 - accuracy: 0.9710 - val_loss: 0.4201 - val_accuracy: 0.8916\n",
      "Epoch 50/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9692\n",
      "Epoch 00050: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0876 - accuracy: 0.9692 - val_loss: 0.4295 - val_accuracy: 0.8858\n",
      "Epoch 51/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9700\n",
      "Epoch 00051: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0848 - accuracy: 0.9700 - val_loss: 0.4027 - val_accuracy: 0.8905\n",
      "Epoch 52/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9693\n",
      "Epoch 00052: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0870 - accuracy: 0.9694 - val_loss: 0.3927 - val_accuracy: 0.8940\n",
      "Epoch 53/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9697\n",
      "Epoch 00053: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0845 - accuracy: 0.9697 - val_loss: 0.4001 - val_accuracy: 0.8932\n",
      "Epoch 54/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9711\n",
      "Epoch 00054: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0835 - accuracy: 0.9710 - val_loss: 0.4248 - val_accuracy: 0.8888\n",
      "Epoch 55/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9697\n",
      "Epoch 00055: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0871 - accuracy: 0.9697 - val_loss: 0.4091 - val_accuracy: 0.8932\n",
      "Epoch 56/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9702\n",
      "Epoch 00056: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0848 - accuracy: 0.9702 - val_loss: 0.3928 - val_accuracy: 0.8965\n",
      "Epoch 57/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9711\n",
      "Epoch 00057: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0836 - accuracy: 0.9711 - val_loss: 0.4275 - val_accuracy: 0.8900\n",
      "Epoch 58/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9723\n",
      "Epoch 00058: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0818 - accuracy: 0.9723 - val_loss: 0.4241 - val_accuracy: 0.8931\n",
      "Epoch 59/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9709\n",
      "Epoch 00059: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0810 - accuracy: 0.9709 - val_loss: 0.4042 - val_accuracy: 0.8948\n",
      "Epoch 60/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9713\n",
      "Epoch 00060: val_accuracy did not improve from 0.89884\n",
      "1562/1562 [==============================] - 87s 55ms/step - loss: 0.0831 - accuracy: 0.9713 - val_loss: 0.4305 - val_accuracy: 0.8885\n",
      "Epoch 61/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9701\n",
      "Epoch 00061: val_accuracy improved from 0.89884 to 0.89934, saving model to training_2/cp-0061.ckpt\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0853 - accuracy: 0.9701 - val_loss: 0.3907 - val_accuracy: 0.8993\n",
      "Epoch 62/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9709\n",
      "Epoch 00062: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0831 - accuracy: 0.9709 - val_loss: 0.4155 - val_accuracy: 0.8913\n",
      "Epoch 63/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9713\n",
      "Epoch 00063: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 86s 55ms/step - loss: 0.0826 - accuracy: 0.9713 - val_loss: 0.4082 - val_accuracy: 0.8950\n",
      "Epoch 64/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9712\n",
      "Epoch 00064: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 86s 55ms/step - loss: 0.0843 - accuracy: 0.9712 - val_loss: 0.4509 - val_accuracy: 0.8872\n",
      "Epoch 65/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9718\n",
      "Epoch 00065: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 86s 55ms/step - loss: 0.0819 - accuracy: 0.9718 - val_loss: 0.4056 - val_accuracy: 0.8964\n",
      "Epoch 66/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9721\n",
      "Epoch 00066: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 86s 55ms/step - loss: 0.0809 - accuracy: 0.9720 - val_loss: 0.4581 - val_accuracy: 0.8804\n",
      "Epoch 67/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9701\n",
      "Epoch 00067: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 86s 55ms/step - loss: 0.0837 - accuracy: 0.9701 - val_loss: 0.4215 - val_accuracy: 0.8976\n",
      "Epoch 68/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9718\n",
      "Epoch 00068: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0803 - accuracy: 0.9718 - val_loss: 0.4290 - val_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9728\n",
      "Epoch 00069: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0809 - accuracy: 0.9728 - val_loss: 0.4162 - val_accuracy: 0.8911\n",
      "Epoch 70/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9713\n",
      "Epoch 00070: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 57ms/step - loss: 0.0822 - accuracy: 0.9713 - val_loss: 0.4127 - val_accuracy: 0.8957\n",
      "Epoch 71/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9718\n",
      "Epoch 00071: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0827 - accuracy: 0.9717 - val_loss: 0.4176 - val_accuracy: 0.8903\n",
      "Epoch 72/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9712\n",
      "Epoch 00072: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0810 - accuracy: 0.9713 - val_loss: 0.4213 - val_accuracy: 0.8937\n",
      "Epoch 73/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9712\n",
      "Epoch 00073: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0830 - accuracy: 0.9711 - val_loss: 0.4269 - val_accuracy: 0.8925\n",
      "Epoch 74/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9732\n",
      "Epoch 00074: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0756 - accuracy: 0.9732 - val_loss: 0.4198 - val_accuracy: 0.8956\n",
      "Epoch 75/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9728\n",
      "Epoch 00075: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0775 - accuracy: 0.9728 - val_loss: 0.4136 - val_accuracy: 0.8965\n",
      "Epoch 76/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9712\n",
      "Epoch 00076: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0808 - accuracy: 0.9713 - val_loss: 0.4306 - val_accuracy: 0.8883\n",
      "Epoch 77/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9709\n",
      "Epoch 00077: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0821 - accuracy: 0.9709 - val_loss: 0.4217 - val_accuracy: 0.8892\n",
      "Epoch 78/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9731\n",
      "Epoch 00078: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0773 - accuracy: 0.9730 - val_loss: 0.4376 - val_accuracy: 0.8924\n",
      "Epoch 79/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9718\n",
      "Epoch 00079: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0812 - accuracy: 0.9718 - val_loss: 0.4374 - val_accuracy: 0.8893\n",
      "Epoch 80/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9731\n",
      "Epoch 00080: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0748 - accuracy: 0.9732 - val_loss: 0.4761 - val_accuracy: 0.8829\n",
      "Epoch 81/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9729\n",
      "Epoch 00081: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0768 - accuracy: 0.9729 - val_loss: 0.4201 - val_accuracy: 0.8958\n",
      "Epoch 82/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9729\n",
      "Epoch 00082: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0793 - accuracy: 0.9729 - val_loss: 0.4204 - val_accuracy: 0.8946\n",
      "Epoch 83/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9732\n",
      "Epoch 00083: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0763 - accuracy: 0.9732 - val_loss: 0.3975 - val_accuracy: 0.8944\n",
      "Epoch 84/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9724\n",
      "Epoch 00084: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0785 - accuracy: 0.9724 - val_loss: 0.4128 - val_accuracy: 0.8960\n",
      "Epoch 85/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9727\n",
      "Epoch 00085: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0779 - accuracy: 0.9728 - val_loss: 0.4176 - val_accuracy: 0.8924\n",
      "Epoch 86/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9723\n",
      "Epoch 00086: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0790 - accuracy: 0.9722 - val_loss: 0.4120 - val_accuracy: 0.8950\n",
      "Epoch 87/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9739\n",
      "Epoch 00087: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0764 - accuracy: 0.9739 - val_loss: 0.4155 - val_accuracy: 0.8936\n",
      "Epoch 88/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9736\n",
      "Epoch 00088: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0753 - accuracy: 0.9736 - val_loss: 0.4174 - val_accuracy: 0.8935\n",
      "Epoch 89/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9736\n",
      "Epoch 00089: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0756 - accuracy: 0.9736 - val_loss: 0.4337 - val_accuracy: 0.8879\n",
      "Epoch 90/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9741\n",
      "Epoch 00090: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0735 - accuracy: 0.9741 - val_loss: 0.4232 - val_accuracy: 0.8952\n",
      "Epoch 91/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9719\n",
      "Epoch 00091: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 89s 57ms/step - loss: 0.0790 - accuracy: 0.9718 - val_loss: 0.4191 - val_accuracy: 0.8933\n",
      "Epoch 92/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9729\n",
      "Epoch 00092: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0775 - accuracy: 0.9729 - val_loss: 0.4693 - val_accuracy: 0.8845\n",
      "Epoch 93/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9743\n",
      "Epoch 00093: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0762 - accuracy: 0.9742 - val_loss: 0.4271 - val_accuracy: 0.8932\n",
      "Epoch 94/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9732\n",
      "Epoch 00094: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0782 - accuracy: 0.9733 - val_loss: 0.4599 - val_accuracy: 0.8847\n",
      "Epoch 95/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9727\n",
      "Epoch 00095: val_accuracy did not improve from 0.89934\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0770 - accuracy: 0.9727 - val_loss: 0.3990 - val_accuracy: 0.8985\n",
      "Epoch 96/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9740\n",
      "Epoch 00096: val_accuracy improved from 0.89934 to 0.90034, saving model to training_2/cp-0096.ckpt\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0771 - accuracy: 0.9740 - val_loss: 0.3798 - val_accuracy: 0.9003\n",
      "Epoch 97/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9729\n",
      "Epoch 00097: val_accuracy did not improve from 0.90034\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0771 - accuracy: 0.9729 - val_loss: 0.4409 - val_accuracy: 0.8873\n",
      "Epoch 98/100\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9743\n",
      "Epoch 00098: val_accuracy did not improve from 0.90034\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0720 - accuracy: 0.9743 - val_loss: 0.4280 - val_accuracy: 0.8902\n",
      "Epoch 99/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9741\n",
      "Epoch 00099: val_accuracy did not improve from 0.90034\n",
      "1562/1562 [==============================] - 87s 56ms/step - loss: 0.0743 - accuracy: 0.9741 - val_loss: 0.4053 - val_accuracy: 0.8976\n",
      "Epoch 100/100\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9738\n",
      "Epoch 00100: val_accuracy improved from 0.90034 to 0.90134, saving model to training_2/cp-0100.ckpt\n",
      "1562/1562 [==============================] - 88s 56ms/step - loss: 0.0734 - accuracy: 0.9738 - val_loss: 0.3840 - val_accuracy: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1780ed4a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path=\"training_2/cp-{epoch:04d}.ckpt\" \n",
    "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1,save_weights_only=True)\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=(len(X_train) // 32),\n",
    "        epochs=100,\n",
    "        validation_data=test_generator,callbacks=[checkpoint],\n",
    "        validation_steps=(len(X_test) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "hV5JDYnwNUow",
    "outputId": "7960fa2c-5705-4f58-ed9a-658ebe9fc07e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.3891 - accuracy: 0.9003\n",
      "Restored model, accuracy: 90.03%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_generator)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pradeepsingam333@gmail.com_22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
